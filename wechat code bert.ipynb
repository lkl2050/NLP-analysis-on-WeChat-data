{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#import xgboost as xgb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import layers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "   BertTokenizerFast,\n",
    "   AutoModelForMaskedLM,\n",
    "   AutoModelForTokenClassification,\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModelForMaskedLM.from_pretrained('ckiplab/albert-tiny-chinese') # or other models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the raw dataset\n",
    "data16 = pd.read_csv(r'/Users/cairo/Google Drive/wechat data/016.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')\n",
    "#topicdata = pd.read_csv(r'C:/Users/Junhao/Google Drive/wechat data/TopicOutcomeAll20Topic.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dataset is the raw data processed with cutting word package jieba(a popular word cutting tool for chinese)\n",
    "#cutword = pd.read_pickle(\"C:/Users/Junhao/Google Drive/wechat data/cut001.pkl\")\n",
    "cutword = pd.read_pickle(\"/Users/cairo/Google Drive/wechat data/cut001.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the dataset with topic modeling output\n",
    "topicdata = pd.read_csv(r'/Users/cairo/Google Drive/wechat data/TopicOutcomeAll20Topic.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')\n",
    "#topicdata = pd.read_csv(r'C:/Users/Junhao/Google Drive/wechat data/TopicOutcomeAll20Topic.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'account',\n",
       " 'author',\n",
       " 'type',\n",
       " 'title',\n",
       " 'summary',\n",
       " 'likeCount',\n",
       " 'clicksCount',\n",
       " 'url',\n",
       " 'publicTime',\n",
       " 'orderNum',\n",
       " 'originalFlag',\n",
       " 'imageUrl',\n",
       " 'sourceUrl',\n",
       " 'videoUrl',\n",
       " 'musicUrl',\n",
       " 'audioUrl',\n",
       " 'updateTime',\n",
       " 'insertTime',\n",
       " 'wordsCut']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cutword.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'account',\n",
       " 'author',\n",
       " 'type',\n",
       " 'title',\n",
       " 'summary',\n",
       " 'likeCount',\n",
       " 'clicksCount',\n",
       " 'url',\n",
       " 'publicTime',\n",
       " 'orderNum',\n",
       " 'originalFlag',\n",
       " 'imageUrl',\n",
       " 'sourceUrl',\n",
       " 'videoUrl',\n",
       " 'musicUrl',\n",
       " 'audioUrl',\n",
       " 'updateTime',\n",
       " 'insertTime',\n",
       " 'topic0',\n",
       " 'topic1',\n",
       " 'topic2',\n",
       " 'topic3',\n",
       " 'topic4',\n",
       " 'topic5',\n",
       " 'topic6',\n",
       " 'topic7',\n",
       " 'topic8',\n",
       " 'topic9',\n",
       " 'topic10',\n",
       " 'topic11',\n",
       " 'topic12',\n",
       " 'topic13',\n",
       " 'topic14',\n",
       " 'topic15',\n",
       " 'topic16',\n",
       " 'topic17',\n",
       " 'topic18',\n",
       " 'topic19']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(topicdata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'account',\n",
       " 'author',\n",
       " 'type',\n",
       " 'title',\n",
       " 'summary',\n",
       " 'content',\n",
       " 'likeCount',\n",
       " 'clicksCount',\n",
       " 'url',\n",
       " 'publicTime',\n",
       " 'orderNum',\n",
       " 'originalFlag',\n",
       " 'imageUrl',\n",
       " 'sourceUrl',\n",
       " 'videoUrl',\n",
       " 'musicUrl',\n",
       " 'audioUrl',\n",
       " 'updateTime',\n",
       " 'insertTime']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data16.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutword1 = cutword.head(1000).wordsCut\n",
    "#cutword3 = cutword.head(1000).title  #i can also try to use only the article titles as bert input \n",
    "\n",
    "cutword2 = data16.head(1000).content.dropna() #use 1000 article content from rawdata as bert input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I need to cut each article to the length of 100 characters because the tokenizer can only deal with a max length of 512\n",
    "#the computation takes too long if the kept article length is long\n",
    "\n",
    "cutword3 = []\n",
    "for i in cutword2:\n",
    "    dd = i[:100]\n",
    "    cutword3.append(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今天最多人讨论的就是姚贝娜去世的新闻，满屏的悼念。但是她离开的杀手，大家是否足够了解？和梅艳芳同被乳腺癌夺取年轻的生命。今天转发一个视频希望更多女性重视这个疾病。［一分钟远离乳腺癌］VIDEO提起乳腺癌，我们总能想起一大串熟悉的名字，陈晓旭、叶凡、蔡琴、秀兰·邓波儿……她们中有的人战胜了乳腺癌，顽强地活着；而有的则被乳腺癌带走了。越来越多的媒体、政要、名人、明星开始关注乳腺癌。世界头号女性杀手乳腺癌每年夺取50万女性的生命，并且全球每年还有120万新发病例。在我国，北京、上海、天津等大城市的统计显示，乳腺癌是我国女性最常见的恶性肿瘤，且发病率呈逐年上升的趋势。上海是乳腺癌高发病地区，年龄最大的',\n",
       " '传递健康资讯 成就美好人生 今天不养生，明天养医生。请将健康观念传递给更多朋友。 更多精彩内容请添加小编个人微信：13861899065山有山的高度，水有水的深度，没必要攀比，每个人都有自己的长处；风有风的自由，云有云的温柔，没必要模仿，每个人都有自己的个性。你认为快乐的，就去寻找；你认为值得的，就去守候；你认为幸福的，就去珍惜。没有不被评说的事，没有不被猜测的人。做最真实最漂亮的自己，依心而行，无憾今生。 人生1条路：走自己的路；人生2件宝：身体好、心不老；人生3种朋友：肯借钱给你、参加你的婚礼、参加你的葬礼；人生有4苦：看不透、舍不得、输不起、放不下。人生5句话：再难也要坚持，再好也要淡泊',\n",
       " '↑↑↑巴西插画师 Butcher Billy 将世界各国政商圈的大爷们变成美漫里的大反派——而且看上去还挺符合他们的。他将这个系列命名为现实世界的“复仇者联盟”。Billy除了是个优秀的插画师，他还是个丧心病狂的淘宝店主：他的网店除了让漫迷和极客们疯狂掏钱包的T恤、环保袋、iphone case等等周边，连BB穿的衣服都不放过，当然，上面印的全是些DC的大坏蛋。MAO AS DARKSEID 毛泽东 饰 达克赛德DARKSEID（达克赛德），黑暗君主。使用反生命方程式的他是个多元宇宙级别的存在，可以控制整个多元宇宙的生命。除去可怕的物理能力外，还有着强大的心灵能力，可以控制整个星球的人，还可以',\n",
       " '传递健康资讯 成就美好人生 今天不养生，明天养医生。请将健康观念传递给更多朋友。 更多精彩内容请添加小编个人微信：13861899065（图为癌症患者绝望的流泪）所有人都想远离癌症、健康长寿，但你想知道最简单、便宜的防癌方法是什么吗？国际抗癌联盟曾发表报告称，全球每年有1200万新发癌症病例，其中高达四成原本可以在生活中防住。生命君采访多位权威专家，为你开出9种最便宜的防癌处方，马上分享给你身边的人吧！你所看到的，也许正是别人所需要的，如果您感觉这篇文章触动了你，同时对您的朋友可能也会有所帮助,请分享给他们喔！更多精彩内容请关注： ↓↓↓【名医话养生】公众微信号：myhys555个人微信号：1',\n",
       " '好贱，好贱的。哈哈！喜欢、请点底部，为一神打打气！']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutword3[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutword3 = pd.Series(cutword3)\n",
    "tokenized = cutword3.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(998,)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutword3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(998, 102)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#padding \n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(998, 102)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#masking\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "input_ids = torch.tensor(input_ids).to(torch.int64)\n",
    "attention_mask = torch.tensor(attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([998, 102])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([998, 102])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeCount = data16.head(1000).likeCount #use likecount as dependent variable \n",
    "\n",
    "df = data16[[\"likeCount\", \"content\"]].head(1000)\n",
    "\n",
    "removeinds = pd.isnull(df).any(1).to_numpy().nonzero()[0]\n",
    "\n",
    "likeCount = [i for j, i in enumerate(likeCount) if j not in removeinds] # remove the two likecount where content is empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_output, test_output = train_test_split(features, likeCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 202.018867\n",
      "R square: -0.119434\n"
     ]
    }
   ],
   "source": [
    "#training error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "reg = LinearRegression().fit(train_features, train_output)\n",
    "#reg.score(train_features, train_labels)\n",
    "preds = reg.predict(train_features)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(np.array(train_output).astype(float), preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "print(\"R square: %f\" % (r2_score(np.array(train_output).astype(float), preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.986</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.667</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3.089</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 24 Jan 2021</td> <th>  Prob (F-statistic):</th> <td>0.000120</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:27:54</td>     <th>  Log-Likelihood:    </th> <td> -3400.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   748</td>      <th>  AIC:               </th> <td>   8234.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    32</td>      <th>  BIC:               </th> <td>1.154e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   715</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.formula.api  import ols\n",
    "import statsmodels.api as sm\n",
    "results = sm.OLS(np.array(train_output).astype(float), train_features.astype(float)).fit()\n",
    "results.summary().tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 242.183767\n",
      "R square: -0.650406\n"
     ]
    }
   ],
   "source": [
    "#test error\n",
    "preds = reg.predict(test_features)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(np.array(test_output).astype(float), preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "print(\"R square: %f\" % (r2_score(np.array(test_output).astype(float), preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
